{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# Uber Data Processing\n#### This notebook fetches data from a table from the 'Glue' database, cleans it, creates fact and dimension tables and writes the data back to S3 bucket and glue database.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "## Section 1: Spark setup and reading data\n\n#### 1.1: Setting up spark session",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 3.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Current idle_timeout is 2800 minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 3.0\nPrevious worker type: G.1X\nSetting new worker type to: G.1X\nPrevious number of workers: 5\nSetting new number of workers to: 5\nAuthenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::703507205356:role/glue-uber-data-etl\nTrying to create a Glue session for the kernel.\nWorker Type: G.1X\nNumber of Workers: 5\nSession ID: 536a76ba-1733-47d2-a0c2-024fb065b734\nJob Type: glueetl\nApplying the following default arguments:\n--glue_kernel_version 0.38.1\n--enable-glue-datacatalog true\nWaiting for session 536a76ba-1733-47d2-a0c2-024fb065b734 to get into ready status...\nSession 536a76ba-1733-47d2-a0c2-024fb065b734 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### 1.2: Reading the data from glue database",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "dyf = glueContext.create_dynamic_frame.from_catalog(database='uber_database_glue_full', table_name='uber_database_s3_full')\ndyf.printSchema()",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n|-- VendorID: long\n|-- tpep_pickup_datetime: timestamp\n|-- tpep_dropoff_datetime: timestamp\n|-- passenger_count: double\n|-- trip_distance: double\n|-- RatecodeID: double\n|-- store_and_fwd_flag: string\n|-- PULocationID: long\n|-- DOLocationID: long\n|-- payment_type: long\n|-- fare_amount: double\n|-- extra: double\n|-- mta_tax: double\n|-- tip_amount: double\n|-- tolls_amount: double\n|-- improvement_surcharge: double\n|-- total_amount: double\n|-- congestion_surcharge: double\n|-- airport_fee: double\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### 1.3: Converting the DynamicFrame to a Spark DataFrame and display a sample of the data\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "trips = dyf.toDF()",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 26,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Section 2: Data cleaning and transformations\n\n#### 2.1: Adding a column named trip_id",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import monotonically_increasing_id\ntrips = trips.withColumn(\"trip_id\", monotonically_increasing_id())",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 27,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### 2.2: Only Keeping or filtering out data that has 2022 as it's year, outliers are left behind",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import year\n\n\n# Create a new column named 'pickup_year'\ntrips = trips.withColumn('pickup_year', year('tpep_pickup_datetime'))\n\n# Filter rows with year 2022\ntrips = trips.filter(trips['pickup_year'] == 2022)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 28,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### 2.3: Dropping Duplicates",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "trips = trips.dropDuplicates()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 29,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### 2.4: Removing unwanted columns",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "trips = trips.drop(*['airport_fee','store_and_fwd_flag', 'congestion_surcharge','extra',\n                     'mta_tax','tolls_amount','improvement_surcharge'])",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 31,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### 2.5: Converting columns to date time",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import col, to_timestamp, dayofmonth\n\ntrips = trips.withColumn(\"tpep_pickup_datetime_converted\", to_timestamp(col(\"tpep_pickup_datetime\"))) \\\n             .withColumn(\"tpep_dropoff_datetime_converted\", to_timestamp(col(\"tpep_dropoff_datetime\")))\n\n# Extract day of the month\ntrips = trips.withColumn(\"pickup_day\", dayofmonth(col(\"tpep_pickup_datetime_converted\"))) \\\n             .withColumn(\"dropoff_day\", dayofmonth(col(\"tpep_dropoff_datetime_converted\")))\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 32,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### 2.6: Creating a new column for day of the week",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import date_format\n\n\ntrips = trips.withColumn('pickup_day_of_week', date_format('tpep_pickup_datetime_converted', 'EEEE'))\n\n# Add dropoff_day_of_week column\ntrips = trips.withColumn('dropoff_day_of_week', date_format('tpep_dropoff_datetime_converted', 'EEEE'))\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 33,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### 2.7: Creating a new column named payment_type_name",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import when, lit\n\npayment_type_name = {\n    1: \"Credit card\",\n    2: \"Cash\",\n    3: \"No charge\",\n    4: \"Dispute\",\n    5: \"Unknown\",\n    6: \"Voided trip\"\n}\n\ntrips = trips.withColumn('payment_type_name', lit(\"Unknown\"))\nfor key, value in payment_type_name.items():\n    trips = trips.withColumn('payment_type_name', when(col('payment_type') == key, value).otherwise(col('payment_type_name')))",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 34,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### 2.8:  Removing rows with fare amount < 0",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "trips = trips.filter(col('fare_amount') >= 0.0)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 35,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### 2.9: Adding columns for Borough using taxi-zone-lookup.csv file",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "s3_path = \"s3://uber-data-engg-bucket/uber_data/TLC_LookupFile/taxi-zone-lookup.csv\"",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 36,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df_taxi_zone_lookup = spark.read.csv(s3_path, header=True, inferSchema=True)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 37,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# If the columns LocationID and Zone are not dropped, it leads to duplicate records after joining.\n# To overcome this problem, the join condition requires to drop columns LocationID and Zone to avoid multiple join columns.\n\n# Creating PUBorough:\ntrips = trips.join(df_taxi_zone_lookup, trips.PULocationID == df_taxi_zone_lookup.LocationID, 'left')\ntrips = trips.withColumnRenamed('Borough', 'PUBorough')\ntrips = trips.drop('LocationID','Zone')\n\n#Creating DUBorough:\ntrips = trips.join(df_taxi_zone_lookup, trips.DOLocationID == df_taxi_zone_lookup.LocationID, 'left')\ntrips = trips.withColumnRenamed('Borough', 'DOBorough')\ntrips = trips.drop('LocationID','Zone')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 38,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Section 3: Data Modeling: Converting dataframe into fact and dimension tables",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### 3.1: datetime_dim table ",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# PySpark equivalent of selecting fixed columns is using .select.\n# Adding a new column is done using .withColumn.\n\nfrom pyspark.sql.functions import hour, dayofmonth, month, year, dayofweek\n\n\n\n# Select the relevant columns and create a new DataFrame 'datetime_dim'\ndatetime_dim = trips.select('tpep_pickup_datetime', 'tpep_dropoff_datetime')\n\n# Extract date and time components\ndatetime_dim = datetime_dim \\\n    .withColumn('pick_hour', hour('tpep_pickup_datetime')) \\\n    .withColumn('pick_day', dayofmonth('tpep_pickup_datetime')) \\\n    .withColumn('pick_month', month('tpep_pickup_datetime')) \\\n    .withColumn('pick_year', year('tpep_pickup_datetime')) \\\n    .withColumn('pick_weekday', dayofweek('tpep_pickup_datetime')) \\\n    .withColumn('drop_hour', hour('tpep_dropoff_datetime')) \\\n    .withColumn('drop_day', dayofmonth('tpep_dropoff_datetime')) \\\n    .withColumn('drop_month', month('tpep_dropoff_datetime')) \\\n    .withColumn('drop_year', year('tpep_dropoff_datetime')) \\\n    .withColumn('drop_weekday', dayofweek('tpep_dropoff_datetime'))\n\n# # Add datetime_id column\ndatetime_dim = datetime_dim.withColumn('datetime_id', monotonically_increasing_id())\n\n# # # Reorder columns\ndatetime_dim = datetime_dim.select('datetime_id', 'tpep_pickup_datetime', 'pick_hour', 'pick_day', 'pick_month', 'pick_year',\n                                   'pick_weekday', 'tpep_dropoff_datetime', 'drop_hour', 'drop_day', 'drop_month',\n                                   'drop_year', 'drop_weekday')\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 40,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### 3.2: passenger_count_dim table",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "passenger_count_dim = trips.select('passenger_count') \\\n    .withColumn('passenger_count_id', monotonically_increasing_id())\n\n# Reorder columns\npassenger_count_dim = passenger_count_dim.select('passenger_count_id', 'passenger_count')\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 41,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### 3.3: trip_distance_dim table",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Select the 'trip_distance' column and add a new column 'trip_distance_id'\ntrip_distance_dim = trips.select('trip_distance') \\\n    .withColumn('trip_distance_id', monotonically_increasing_id())\n\n# Reorder columns\ntrip_distance_dim = trip_distance_dim.select('trip_distance_id', 'trip_distance')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 42,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### 3.4: rate_code_dim table",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "rate_code_type = {\n    1:\"Standard rate\",\n    2:\"JFK\",\n    3:\"Newark\",\n    4:\"Nassau or Westchester\",\n    5:\"Negotiated fare\",\n    6:\"Group ride\"\n}\n\nrate_code_dim = trips.select('RatecodeID').withColumn('rate_code_id', monotonically_increasing_id())\n\n\n\nrate_code_dim = rate_code_dim.withColumn('rate_code_name', lit(\"\"))\nfor key, value in rate_code_type.items():\n    rate_code_dim = rate_code_dim.withColumn('rate_code_name', when(col('RatecodeID') == key, value)\\\n                                             .otherwise(col('rate_code_name')))",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 43,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### 3.5: payment_type_dim table",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "payment_type_name = {\n    1:\"Credit card\",\n    2:\"Cash\",\n    3:\"No charge\",\n    4:\"Dispute\",\n    5:\"Unknown\",\n    6:\"Voided trip\"\n}\n\npayment_type_dim = trips.select('payment_type')\\\n                .withColumn('payment_type_id', monotonically_increasing_id())\n\n\n\npayment_type_dim = payment_type_dim.withColumn('payment_type_name', lit(\"\"))\nfor key, value in payment_type_name.items():\n    payment_type_dim = payment_type_dim.withColumn('payment_type_name', when(col('payment_type') == key, value)\\\n                                             .otherwise(col('payment_type_name')))",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 44,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### 3.6: pickup_location_dim table",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "pickup_location_dim = trips.select('PULocationID', 'PUBorough') \\\n    .withColumn('pickup_location_id', monotonically_increasing_id())\n\n# Reorder columns\npickup_location_dim = pickup_location_dim.select('pickup_location_id', 'PULocationID', 'PUBorough')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 45,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### 3.7: dropoff_location_dim table",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "dropoff_location_dim = trips.select('DOLocationID', 'DOBorough') \\\n    .withColumn('dropoff_location_id', monotonically_increasing_id())\n\n# Reorder columns\ndropoff_location_dim = dropoff_location_dim.select('dropoff_location_id', 'DOLocationID', 'DOBorough')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 46,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### 3.8: fact_table",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "fact_table = trips \\\n    .join(passenger_count_dim, trips['trip_id'] == passenger_count_dim['passenger_count_id'], 'inner') \\\n    .join(trip_distance_dim, trips['trip_id'] == trip_distance_dim['trip_distance_id'], 'inner') \\\n    .join(rate_code_dim, trips['trip_id'] == rate_code_dim['rate_code_id'], 'inner') \\\n    .join(datetime_dim, trips['trip_id'] == datetime_dim['datetime_id'], 'inner') \\\n    .join(payment_type_dim, trips['trip_id'] == payment_type_dim['payment_type_id'], 'inner') \\\n    .join(pickup_location_dim, trips['trip_id'] == pickup_location_dim['pickup_location_id'], 'inner') \\\n    .join(dropoff_location_dim, trips['trip_id'] == dropoff_location_dim['dropoff_location_id'], 'inner') \\\n    .select('trip_id', 'VendorID', 'datetime_id', 'passenger_count_id', 'trip_distance_id', \n            'rate_code_id', 'payment_type_id', 'fare_amount', 'tip_amount', 'total_amount', 'pickup_location_id',\n            'dropoff_location_id')\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 47,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Section 4: Write the data in the DynamicFrame to a location in Amazon S3 and a table for it in the AWS Glue Data Catalog",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### 4.1: Converting PySpark DataFrame to a Glue DynamicFrame",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Convert dataframe back to dynamicframe:\nfrom awsglue.dynamicframe import DynamicFrame\nfact_table = DynamicFrame.fromDF(fact_table, glueContext, \"fact_table\")\npassenger_count_dim = DynamicFrame.fromDF(passenger_count_dim, glueContext, \"passenger_count_dim\")\ntrip_distance_dim = DynamicFrame.fromDF(trip_distance_dim, glueContext, \"trip_distance_dim\")\nrate_code_dim = DynamicFrame.fromDF(rate_code_dim, glueContext, \"rate_code_dim\")\ndatetime_dim = DynamicFrame.fromDF(datetime_dim, glueContext, \"datetime_dim\")\npayment_type_dim = DynamicFrame.fromDF(payment_type_dim, glueContext, \"payment_type_dim\")\npickup_location_dim = DynamicFrame.fromDF(pickup_location_dim, glueContext, \"pickup_location_dim\")\ndropoff_location_dim = DynamicFrame.fromDF(dropoff_location_dim, glueContext, \"dropoff_location_dim\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 48,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### 4.2: Creating a dictionary of dynamic frames",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "dict_of_tables = {'fact_table': fact_table, \n                  'passenger_count_dim': passenger_count_dim, \n                  'trip_distance_dim': trip_distance_dim,\n                  'rate_code_dim': rate_code_dim, \n                  'datetime_dim': datetime_dim, \n                  'payment_type_dim': payment_type_dim,\n                  'pickup_location_dim':pickup_location_dim,\n                  'dropoff_location_dim':dropoff_location_dim}",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 49,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### 4.3: Defining a function to write the data into S3 bucket and to also dump it into a glue database",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "import os\n\nfor name, table in dict_of_tables.items():\n    print(name, table)\n    \n    # Applying repartition eliminates the parallel processing power of Spark. Commenting the below code to improve performance\n    # Repartition to a single partition\n    # table = table.repartition(1)\n    output_path = f\"s3://uber-data-engg-bucket/uber_data/Output_ETL_full/athena_output_parquet_full/{name}\"\n\n    # Check if the path exists\n    if not os.path.exists(output_path):\n        # Create the directory if it doesn't exist\n        os.makedirs(output_path)\n        \n    s3output = glueContext.getSink(\n                                      path=output_path,\n                                      connection_type=\"s3\",\n                                      updateBehavior=\"UPDATE_IN_DATABASE\",\n                                      partitionKeys=[],\n                                      compression=\"snappy\",\n                                      enableUpdateCatalog=True,\n                                      transformation_ctx=\"s3output\",\n                                    )\n    \n    s3output.setCatalogInfo(\n      catalogDatabase=\"uber_database_glue_full\", catalogTableName=f\"etl_uber_{name}_full\"\n    )\n    s3output.setFormat(\"glueparquet\")\n    s3output.writeFrame(table)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 50,
			"outputs": [
				{
					"name": "stdout",
					"text": "fact_table <awsglue.dynamicframe.DynamicFrame object at 0x7f56f9f49290>\n<awsglue.dynamicframe.DynamicFrame object at 0x7f56f9f46590>\npassenger_count_dim <awsglue.dynamicframe.DynamicFrame object at 0x7f56f9f49610>\n<awsglue.dynamicframe.DynamicFrame object at 0x7f56f9f67e50>\ntrip_distance_dim <awsglue.dynamicframe.DynamicFrame object at 0x7f56f9f496d0>\n<awsglue.dynamicframe.DynamicFrame object at 0x7f56fb4a0350>\nrate_code_dim <awsglue.dynamicframe.DynamicFrame object at 0x7f56f9f49350>\n<awsglue.dynamicframe.DynamicFrame object at 0x7f56fb4a0750>\ndatetime_dim <awsglue.dynamicframe.DynamicFrame object at 0x7f56f9f49710>\n<awsglue.dynamicframe.DynamicFrame object at 0x7f56f9f67b90>\npayment_type_dim <awsglue.dynamicframe.DynamicFrame object at 0x7f56f9f49590>\n<awsglue.dynamicframe.DynamicFrame object at 0x7f56f9f67b10>\npickup_location_dim <awsglue.dynamicframe.DynamicFrame object at 0x7f56f9f49550>\n<awsglue.dynamicframe.DynamicFrame object at 0x7f56f9f674d0>\ndropoff_location_dim <awsglue.dynamicframe.DynamicFrame object at 0x7f56f9f495d0>\n<awsglue.dynamicframe.DynamicFrame object at 0x7f56f9f6bc90>\n",
					"output_type": "stream"
				}
			]
		}
	]
}